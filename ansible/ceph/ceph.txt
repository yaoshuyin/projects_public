.使用cephfs供多机共享挂载
   1) on ceph master
   ~/cluster$  ceph-deploy --overwrite-conf mds create ceph-master

   ~/cluster$  ceph osd pool create pool1 10
   ~/cluster$  ceph osd pool create pool1_metadata 10
   ~/cluster$  ceph fs new cephfs pool1_metadata pool1

   ~/cluster$  ceph fs ls
   ~/cluster$  ceph mds stat
   ~/cluster$  grep 'key =' /etc/ceph/ceph.client.admin.keyring | awk -F '=' '{gsub(" ","",$2);print $2}' > admin.secret

   2) on ceph client
   a) mount
   ~/cluster$  sudo mkdir /mnt/test1
   ~/cluster$  sudo mount -t ceph 192.168.1.213:6789:/ /mnt/test1 -o name=admin,secretfile=admin.secret

   ~/cluster$  sudo mkdir /mnt/test2
   ~/cluster$  sudo mount -t ceph 192.168.1.212:6789:/ /mnt/test2 -o name=admin,secretfile=admin.secret

   b) test
   ~/cluster$  sudo touch /mnt/test1/999
   ~/cluster$  ls -l /mnt/test1/999
               16:26 /mnt/test1/999

   ~/cluster$  ls -l /mnt/test2/999
               16:26 /mnt/test2/999

   ~/cluster$  sudo touch /mnt/test2/888
   ~/cluster$  ls -l /mnt/test1/888
               16:27 /mnt/test1/888

   ~/cluster$  ls -l /mnt/test2/888
               16:27 /mnt/test2/888

..........................................

.libceph-common.so.0: cannot map zero-fill pages
 服务器内存不足，需要添加内存

........................................

.ceph osd tree 出现  osd.2  down
a)在master上执行./remove.sh
#!/bin/sh

osd_id=`ceph osd tree | grep down| awk '{print $1}'`
for var in ${osd_id}
do
        echo "deleting osd.$var"
        ceph osd out osd.$var 
        ceph osd rm osd.$var 
        ceph osd crush rm osd.$var 
        ceph auth del osd.$var
        sleep 5
        echo "umount /var/lib/ceph/osd/ceph-$var "
        sudo umount /var/lib/ceph/osd/ceph-$var
done

b)在osd.2上
$ sudo ls -1 /var/lib/ceph/osd/
ceph-2

$ umount /var/lib/ceph/osd/ceph-2

$ wipefs -af /dev/vg-data/lv-data
$ sudo reboot

$ ceph-volume lvm zap /dev/vg-data/lv-data

c)在master上
~/cluster$  ceph-deploy osd create --bluestore ceph-osd2 --data /dev/vg-data/lv-data

d)查看现在状态
~/cluster$ $ ceph osd treecep
 0   hdd 0.01949    osd.0    up    1.00000  1.00000
 1   hdd 0.01949    osd.1    up    1.00000  1.00000
 2   hdd 0.01949    osd.2    up    1.00000  1.00000
