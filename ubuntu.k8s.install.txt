
.环境Ubunt18.06 X86_64 (macos 下的 multipass)

.设置可以访问国外的代理 （所有节点)
  root@k8s-master:~# export no_proxy="localhost,192.168.0.0/16,10.0.0.0/8,127.0.0.0,127.0.1.1,127.0.1.1,local.home,*aliyuncs.com"
  root@k8s-master:~# export http_proxy=http://192.168.1.88:8889/
  root@k8s-master:~# export https_proxy=http://192.168.1.88:8889/

  root@k8s-master:~# export proxy_none_ip="localhost,192.168.0.0/16,10.0.0.0/8,127.0.0.0,127.0.1.1,127.0.1.1,local.home,*aliyuncs.com"
  root@k8s-master:~# export proxy_ip=http://192.168.1.88:8889/

.更新安装docker （所有节点)
  root@k8s-master:~# apt -y update
  root@k8s-master:~# apt -y upgrade
  root@k8s-master:~# apt install -y docker.io
  root@k8s-master:~# systemctl enable docker
  root@k8s-master:~# systemctl start docker
  root@k8s-master:~# systemctl status docker

.禁用swap (所有节点)
 Kubernetes v1.8+ 要求关闭系统 Swap
  1）修改/etc/fstab 文件，它永久性地关闭swap space, 只需在下面两行前面加上（#）
  2）swapoff -a

.安装kubeadm  （所有节点)
  root@k8s-master:~# kubeadm reset (如果需要)
  root@k8s-master:~# curl -v -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add
  root@k8s-master:~# apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"

  root@k8s-master:~# apt -y install kubeadm kubelet kubectl

  root@k8s-master:~# systemctl daemon-reload
  root@k8s-master:~# systemctl enable kubelet

.在master 节点启动集群（master节点）
 这一步会自动执行 systemctl start kubelet
 root@k8s-master:~# kubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap --image-repository registry.aliyuncs.com/google_containers

.创建用户k8s （主节点)
  root@k8s-master:~# useradd -m -s /bin/bash k8s
  root@k8s-master:~# vim /etc/sudoers
    root    ALL=(ALL:ALL) ALL
    k8s     ALL=(ALL:ALL) NOPASSWD:ALL

  #root@k8s-master:~# su - k8s
  root@k8s-master:~# echo "source <(kubectl completion bash)" >> ~/.bashrc

  root@k8s-master:~# mkdir -p $HOME/.kube
  root@k8s-master:~# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  root@k8s-master:~# sudo chown $(id -u):$(id -g) $HOME/.kube/config

  root@k8s-master:~# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

  root@k8s-master:~# echo "1" >/proc/sys/net/bridge/bridge-nf-call-iptables

  root@k8s-master:~# kubectl get pods,svc -n kube-system -o wide
     NAME                                     READY   STATUS    RESTARTS   AGE     IP             NODE
     pod/coredns-9d85f5447-9jjzh              1/1     Running   1          4h4m    10.244.0.14    k8s-master
     pod/coredns-9d85f5447-kjcz5              1/1     Running   1          4h4m    10.244.0.15    k8s-master
     pod/etcd-k8s-master                      1/1     Running   1          4h4m    192.168.64.6   k8s-master
     pod/kube-apiserver-k8s-master            1/1     Running   1          4h4m    192.168.64.6   k8s-master
     pod/kube-controller-manager-k8s-master   1/1     Running   1          4h4m    192.168.64.6   k8s-master
     pod/kube-flannel-ds-amd64-fcj5v          1/1     Running   1          3h16m   192.168.64.7   k8s-node1
     pod/kube-flannel-ds-amd64-mpsvh          1/1     Running   1          3h52m   192.168.64.6   k8s-master
     pod/kube-proxy-fmn6f                     1/1     Running   1          4h4m    192.168.64.6   k8s-master
     pod/kube-proxy-r477f                     1/1     Running   0          3h16m   192.168.64.7   k8s-node1
     pod/kube-scheduler-k8s-master            1/1     Running   1          4h4m    192.168.64.6   k8s-master

     NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE    SELECTOR
     service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   4h4m   k8s-app=kube-dns

  root@k8s-master:~# kubectl get pods -n kube-system
     NAME                                 READY   STATUS    RESTARTS   AGE
     coredns-9d85f5447-9jjzh              1/1     Running   1          4h7m
     coredns-9d85f5447-kjcz5              1/1     Running   1          4h7m
     etcd-k8s-master                      1/1     Running   1          4h7m
     kube-apiserver-k8s-master            1/1     Running   1          4h7m
     kube-controller-manager-k8s-master   1/1     Running   1          4h7m
     kube-flannel-ds-amd64-fcj5v          1/1     Running   1          3h19m
     kube-flannel-ds-amd64-mpsvh          1/1     Running   1          3h55m
     kube-proxy-fmn6f                     1/1     Running   1          4h7m
     kube-proxy-r477f                     1/1     Running   0          3h19m
     kube-scheduler-k8s-master            1/1     Running   1          4h7m

默认的master节点是不能调度应用pod的，所以我们还需要给master节点打一个污点标记
root@k8s-master:~# kubectl taint nodes --all node-role.kubernetes.io/master-

#获取Node节点加入集群时需要执行的命令（过一段时间就会更新，所以需要重新获取）
root@k8s-master:~# kubeadm token create --print-join-command
  kubeadm join 192.168.64.6:6443 --token trj9xj.2hagbra4f4muskho --discovery-token-ca-cert-hash sha256:e748897ee9657323daedf3862e712c6085ce3c2ab047189e91e9bcf45acdb003

root@k8s-master:~#  kubectl cluster-info
   Kubernetes master is running at https://192.168.64.6:6443
   KubeDNS is running at https://192.168.64.6:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

.Node节点加入集群 (Node节点,会自动systemctl start kubelet)
  kubeadm join 192.168.64.6:6443 --token trj9xj.2hagbra4f4muskho --discovery-token-ca-cert-hash sha256:e748897ee9657323daedf3862e712c6085ce3c2ab047189e91e9bcf45acdb003

root@k8s-master:~# kubectl get pods --namespace=kube-system
            NAME                        READY   STATUS    RESTARTS   AGE
    coredns-9d85f5447-9jjzh              1/1     Running   0          95m
    coredns-9d85f5447-kjcz5              1/1     Running   0          95m
    etcd-k8s-master                      1/1     Running   0          95m
    kube-apiserver-k8s-master            1/1     Running   0          95m
    kube-controller-manager-k8s-master   1/1     Running   0          95m
    kube-flannel-ds-amd64-fcj5v          1/1     Running   1          47m
    kube-flannel-ds-amd64-mpsvh          1/1     Running   0          83m
    kube-proxy-fmn6f                     1/1     Running   0          95m
    kube-proxy-r477f                     1/1     Running   0          47m
    kube-scheduler-k8s-master            1/1     Running   0          95m

................................

.安装dashboard (master节点)
#官方: https://github.com/kubernetes/dashboard

#下载dashboard配置文件
root@k8s-master:~# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml

#删除旧的dashboard
root@k8s-master:~# kubectl delete -f admin-user-role-binding.yaml

#创建dashborad
root@k8s-master:~# kubectl apply -f recommended.yaml





..................登陆用户配置.............................

#启动
root@k8s-master:~# kubectl proxy --address='0.0.0.0' --port=8001 --accept-hosts='^*$' &
   http://192.168.64.6:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

#获取状态
root@k8s-master:~# kubectl get pods --namespace=kubernetes-dashboard -o wide

#修改Type: ClusterIP 为 Type: NodePort
root@k8s-master:~# kubectl --namespace=kubernetes-dashboard edit service kubernetes-dashboard

#查看状态
root@k8s-master:~# kubectl --namespace=kubernetes-dashboard get service kubernetes-dashboard

#生成sslkey
root@k8s-master:~# openssl rand -writerand .rnd
root@k8s-master:~# mkdir key
root@k8s-master:~# cd key/
root@k8s-master:~# openssl genrsa -out dashboard.key 2048

#浏览器所在的IP地址 192.168.1.88
root@k8s-master:~# openssl req -new -out dashboard.csr -key dashboard.key -subj '/CN=192.168.1.88'

root@k8s-master:~# openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt
root@k8s-master:~# kubectl delete secret kubernetes-dashboard-certs -n kubernetes-dashboard
root@k8s-master:~# kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard
root@k8s-master:~# kubectl get pod -n kubernetes-dashboard
   NAME                                         READY   STATUS    RESTARTS   AGE
   dashboard-metrics-scraper-7b8b58dc8b-r8j24   1/1     Running   1          54m
   kubernetes-dashboard-5f5f847d57-2kn5t        1/1     Running   0          38m

root@k8s-master:~/k8s/apps#  kubectl get nodes
NAME         STATUS   ROLES    AGE     VERSION
k8s-master   Ready    master   12h     v1.17.4
k8s-node1    Ready    <none>   11h     v1.17.4
k8s-node2    Ready    <none>   7h34m   v1.17.4

#重启pod
root@k8s-master:~/k8s/apps#  kubectl get pod kubernetes-dashboard-5f5f847d57-d8dsg  -n kubernetes-dashboard -o yaml | kubectl replace --force -f -

root@k8s-master:~/k8s/apps#  kubectl --namespace=kubernetes-dashboard get service kubernetes-dashboard


添加用户
$ vim admin-user.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard


$ vim admin-user-role-binding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard

kubectl delete -f admin-user.yaml
kubectl create -f admin-user.yaml

kubectl delete -f admin-user-role-binding.yaml
kubectl create -f admin-user-role-binding.yaml

#获取admin-user的token
$ sudo kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')

#获取登陆端口 :32662
$ sudo kubectl --namespace=kubernetes-dashboard get service kubernetes-dashboardNAME
        TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
kubernetes-dashboard   NodePort      10.99.60.41   <none>     443:32662/TCP   16m

#使用safari浏览器访问 (选择token, 并填入上面获取的token)
 https://192.168.64.6:32662


root@k8s-master:~/k8s/apps# kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://192.168.64.6:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED


root@k8s-master:~/k8s/apps# kubeadm config images list
   W0324 19:37:15.490707   22194 validation.go:28] Cannot validate kube-proxy config - no validator is available
   W0324 19:37:15.492392   22194 validation.go:28] Cannot validate kubelet config - no validator is available
   k8s.gcr.io/kube-apiserver:v1.17.4
   k8s.gcr.io/kube-controller-manager:v1.17.4
   k8s.gcr.io/kube-scheduler:v1.17.4
   k8s.gcr.io/kube-proxy:v1.17.4
   k8s.gcr.io/pause:3.1
   k8s.gcr.io/etcd:3.4.3-0
   k8s.gcr.io/coredns:1.6.5

................应用调用结构........................
  Ingress:80 --->
     [ Node1:32143 ,  Node2:32143 ] -->
         VipService/ClusterIP:80 -->
              [Node1PodsWeb:8081, Node2PodsWeb:8081]

.部署nginx
 1)创建docker
 root@k8s-master:~/k8s/apps#  kubectl create deployment chat-nginx --image=nginx

 root@k8s-master:~/k8s/apps#  kubectl get deployments
          NAME        READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   3/3     3            3           7h28m
    nginx2             1/1     1            1           3m42s

  2)创建同名的service
  root@k8s-master:~/k8s/apps# kubectl create service nodeport chat-nginx --tcp 80:80
      service/chat-nginx created

  root@k8s-master:~/k8s/apps# kubectl get svc
     NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
     chat-nginx   NodePort    10.107.88.44   <none>        80:31057/TCP   25s

root@k8s-master:~/k8s/apps# netstat -tnlp
     Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
     ...
     tcp6       0      0 :::31057                :::*                    LISTEN      3706/kube-proxy
     ...

root@k8s-master:~/k8s/apps# ifconfig
   cni0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1450
           inet 10.244.0.1  netmask 255.255.255.0  broadcast 0.0.0.0

   docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500
           inet 172.17.0.1  netmask 255.255.0.0  broadcast 172.17.255.255

   enp0s2: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
           inet 192.168.64.6  netmask 255.255.255.0  broadcast 192.168.64.255

在外部使用
root@hostA: curl http://192.168.64.6:31057

................pods yaml............................

.使用yaml部署容器
1)打包应用成docker image
  a)编写server.js
var http = require('http');

var handleRequest = function(request, response) {
  console.log('Received request for URL: ' + request.url);
  response.writeHead(200);
  response.end('Hello World!');
};
var www = http.createServer(handleRequest);
www.listen(8081);

  b)打包server.js到docker image
FROM node:8.11.2
WORKDIR app
COPY . .
EXPOSE 8081
ENTRYPOINT [ "node","server.js" ]
Dockerfile创建好后，执行如下命令

  c)打包
root@k8s-master:~/k8s/apps# docker build -t userNameX/kube-node-demo1:v1 .
root@k8s-master:~/k8s/apps# docker images
REPOSITORY                                 TAG                 IMAGE ID            CREATED             SIZE
userNameX/kube-node-demo1                   v1                  e2522bf8e003        1 hours ago        673 MB

root@k8s-master:~/k8s/apps# docker login --username userNameX
Password:
Login Succeeded

root@k8s-master:~/k8s/apps# docker push userNameX/kube-node-demo1:v1
.....

2) 编写deployment.yaml
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-node
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: kube-node-demo-instance
          image: userNameX/kube-node-demo1:v1
          ports:
            - containerPort: 8081

3)执行deployment
root@k8s-master:~/k8s/apps# kubectl create -f deployment.yaml
  deployment.extensions/kube-node created

root@k8s-master:~/k8s/apps# kubectl get pods
  NAME                            READY     STATUS             RESTARTS   AGE
  kube-node-59bf664cbf-2qzgd      0/1       ImagePullBackOff   0          9s
  kube-node-59bf664cbf-p6wtg      0/1       ImagePullBackOff   0          9s

 结果发现Pods的状态为ImagePullBackOff, 最终发现是，因为k8s不能够拉取私有仓库的镜像，
 通过执行下列命令可以查看具体的Pod信息，其中里面的Events下面显示了创建Pod的详细过程。

root@k8s-master:~/k8s/apps# kubectl describe pods/kube-node-59bf664cbf-2qzgd
 参考Kubernetes配置secret拉取私有仓库镜像的官方文档后(点击这里查看)，执行如下命令

root@k8s-master:~/k8s/apps# kubectl create secret docker-registry myregistrykey --docker-server=https://index.docker.io/v1/ --docker-username=userNameX --docker-password=xxxx--docker-email=xxxx@qq.com

root@k8s-master:~/k8s/apps# kubectl get secrets
  NAME                  TYPE                                  DATA      AGENAME                  TYPE                                  DATA      AGE
  default-token-2lvth   kubernetes.io/service-account-token   3         4d
  myregistrykey         kubernetes.io/dockerconfigjson        1         4h
  这样我们就创建好了secret，然后再将这个secret加到yaml文件中，修改后的deployment.yaml文件如下：

---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: kube-node
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: kube-node-demo-instance
          image: kube-node-demo1
          ports:
            - containerPort: 8081
      imagePullSecrets:
       - name: myregistrykey

 删除之前的deployment并重新创建新的deployment
（另一方法是 ：直接编辑此deployment，
   命令是：kubectl edit deployments/kube-node，编辑完后就立即生效了，
   所有pods会重新生成）,命令如下：

root@k8s-master:~/k8s/apps# kubectl delete deployments/kube-node

root@k8s-master:~/k8s/apps# kubectl create -f deployment.yaml
   最终可以看到我们的应用程序被部署上去了

root@k8s-master:~/k8s/apps#  kubectl get pods -o wide
   NAME                            READY     STATUS    RESTARTS   AGE       IP            NODE
   kube-node-7bd98cf84d-c6q6h      1/1       Running   0          3m        10.244.2.66   kube-slave-1
   kube-node-7bd98cf84d-fbjwz      1/1       Running   0          3m        10.244.1.36   kube-slave-3

执行下列命令，来验证部署上去的应用程序是否好用，我们尝试访问第一个IP+8081端口，正常显示Hello World!

root@k8s-master:~/k8s/apps# curl 10.244.2.66:8081
  Hello World!
  到这里，我们的应用程序部署成功了。接下来我们要创建一个service来供外部来访问我们的应用程序

...........service.yaml.................
1)root@k8s-master:~/k8s/apps# vim service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: kube-node-service
  labels:
    name: kube-node-service
spec:
  type: NodePort
  ports:
  - port: 80          #这里的端口和clusterIP(10.97.114.36)对应，即10.97.114.36:80,供内部访问。
    targetPort: 8081  #端口一定要和container暴露出来的端口对应，nodejs暴露出来的端口是8081，所以这里也应是8081
    protocol: TCP
    nodePort: 32143   #所有的节点都会开放此端口，此端口供外部调用。
  selector:
    app: web          #这里选择器一定要选择容器的标签，之前写name:kube-node是错的。
 执行命令创建一个service,并列出所有的services

root@k8s-master:~/k8s/apps# kubectl create -f service.yaml
   service/kube-node-service created

root@k8s-master:~/k8s/apps# kubectl get services
   NAME                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
   kube-node-service   NodePort    10.97.114.36   <none>        80:32143/TCP   7s
   kubernetes          ClusterIP   10.96.0.1      <none>        443/TCP        7d

root@k8s-master:~/k8s/apps# curl localhost:32143
   Hello World!


#由于定义的port是80，所以直接访问clusterIP
root@k8s-master:~/k8s/apps# curl 10.97.114.36
   Hello World!

#在每个节点查看端口执行netstat -ntlp，发现每个节点都开放出了端口32143，此端口主要是给外部分用户调用的

#在master节点上查看slave node的ip
root@k8s-master:~/k8s/apps# kubectl get nodes -o wide
   NAME           STATUS    ROLES     AGE       VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
   kube-master    Ready     master    7d        v1.11.1   192.168.29.138   <none>        Ubuntu 16.04.4 LTS   4.13.0-36-generic   docker://17.3.2
   kube-slave-1   Ready     <none>    1d        v1.11.1   192.168.29.141   <none>        Ubuntu 16.04.4 LTS   4.13.0-36-generic   docker://17.3.2
   kube-slave-3   Ready     <none>    7d        v1.11.1   192.168.29.139   <none>        Ubuntu 16.04.4 LTS   4.15.0-30-generic   docker://17.3.2

#在master节点上,查看所有pods
root@k8s-master:~/k8s/apps# kubectl get pods -o wide
   NAME                            READY     STATUS    RESTARTS   AGE       IP            NODE
   kube-node-64f4f68d4b-sttz2      1/1       Running   0          15m       10.244.1.71   kube-slave-3
   kube-node-64f4f68d4b-vwwt2      1/1       Running   0          1d        10.244.1.69   kube-slave-3

#在master节点上,访问两个pods,端口都是8081
root@k8s-master:~/k8s/apps# curl 10.244.1.71:8081
   Hello World!

root@k8s-master:~/k8s/apps# curl 10.244.1.69:8081
   Hello World!

#在slave节点kube-slave-1上访问32143端口
root@k8s-master:~/k8s/apps# curl 192.168.29.141:32143
   Hello World!

#在slave节点kube-slave-3上访问32143端口
root@k8s-master:~/k8s/apps# curl 192.168.29.139:32143
   Hello World!

.................expose命令来创建service...................

root@k8s-master:~/k8s/apps# kubectl expose deployment kube-node --type=NodePort
  service/kube-node exposed

root@k8s-master:~/k8s/apps# kubectl get services
  NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
  kube-node           NodePort    10.107.247.158   <none>        8081:31195/TCP   10s
  kube-node-service   NodePort    10.97.114.36     <none>        80:32143/TCP     1d
  kubernetes          ClusterIP   10.96.0.1        <none>        443/TCP          8d
  nginx               NodePort    10.99.22.64      <none>        80:32322/TCP     2d

root@k8s-master:~/k8s/apps# curl localhost:31195
  Hello World!

root@k8s-master:~/k8s/apps# curl 10.107.247.158:8081
  Hello World!

root@k8s-master:~/k8s/apps# kubectl describe services/kube-node
  Name:                     kube-node
  Namespace:                default
  Labels:                   app=web
  Annotations:              <none>
  Selector:                 app=web
  Type:                     NodePort
  IP:                       10.107.247.158
  Port:                     <unset>  8081/TCP
  TargetPort:               8081/TCP
  NodePort:                 <unset>  31195/TCP
  Endpoints:                10.244.1.71:8081,10.244.2.81:8081  #这里podIp+端口号就是endpoint
  Session Affinity:         None
  External Traffic Policy:  Cluster
  Events:                   <none>

root@k8s-master:~/k8s/apps# kubectl describe services/kube-node-service
  Name:                     kube-node-service
  Namespace:                default
  Labels:                   name=kube-node-service
  Annotations:              <none>
  Selector:                 app=web
  Type:                     NodePort
  IP:                       10.97.114.36
  Port:                     <unset>  80/TCP
  TargetPort:               8081/TCP
  NodePort:                 <unset>  32143/TCP
  Endpoints:                10.244.1.71:8081,10.244.2.81:8081 #虽然暴露了两个service，但endpoint是一样滴。
  Session Affinity:         None
  External Traffic Policy:  Cluster
  Events:                   <none>
 可以看出，expose命令这里并没有指定clusterIP，说明它是K8S系统中的虚拟IP地址，由系统动态分配。
 Pod的IP地址是由flannel插件来分配的，而不再由Docker Daemon根据docker0网桥的IP地址进行分配。
 可以在任意节点上输入ifconfig可以看到。

......................使用yaml文件创建Service（LoadBalancer）.......................
root@k8s-master:~/k8s/apps# kubectl expose deployment kube-node --type=LoadBalancer
   service/kube-node exposed

root@k8s-master:~/k8s/apps# kubectl get services
   NAME                TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
   kube-node           LoadBalancer   10.99.201.195   <pending>     8081:31585/TCP   10s
   kube-node-service   NodePort       10.97.114.36    <none>        80:32143/TCP     1d
   kubernetes          ClusterIP      10.96.0.1       <none>        443/TCP          8d
   nginx               NodePort       10.99.22.64     <none>        80:32322/TCP     2d

root@k8s-master:~/k8s/apps# curl localhost:31585
   Hello World!

root@k8s-master:~/k8s/apps# curl 10.99.201.195:8081
   Hello World!

root@k8s-master:~/k8s/apps# kubectl describe services/kube-node
   Name:                     kube-node
   Namespace:                default
   Labels:                   app=web
   Annotations:              <none>
   Selector:                 app=web
   Type:                     LoadBalancer
   IP:                       10.99.201.195
   Port:                     <unset>  8081/TCP
   TargetPort:               8081/TCP
   NodePort:                 <unset>  31585/TCP
   Endpoints:                10.244.1.71:8081,10.244.2.81:8081
   Session Affinity:         None
   External Traffic Policy:  Cluster
   Events:                   <none>


root@k8s-master:~/k8s/apps# vim service-lb.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: kube-node-service-lb
  labels:
    name: kube-node-service-lb
spec:
  type: LoadBalancer
  clusterIP: 10.99.201.198
  ports:
  - port: 80
    targetPort: 8081
    protocol: TCP
    nodePort: 32145
  selector:
    app: web
status:
  loadBalancer:
    ingress:
    - ip: 192.168.174.127    #这里是云服务商提供的负载匀衡器的IP地址

root@k8s-master:~/k8s/apps# kubectl create -f service-lb.yaml
   service/kube-node-service-lb created

root@k8s-master:~/k8s/apps# kubectl get services
   NAME                   TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
   kube-node              LoadBalancer   10.99.201.195   <pending>     8081:31585/TCP   21m
   kube-node-service      NodePort       10.97.114.36    <none>        80:32143/TCP     1d
   kube-node-service-lb   LoadBalancer   10.99.201.198   <pending>     80:32145/TCP     9s
   kubernetes             ClusterIP      10.96.0.1       <none>        443/TCP          8d
   nginx                  NodePort       10.99.22.64     <none>        80:32322/TCP     2d

root@k8s-master:~/k8s/apps# curl 10.99.201.195:8081
  Hello World!

root@k8s-master:~/k8s/apps# curl localhost:31585
  Hello World!


..........................................................
1、service资源实现模型
（1）service资源

       Service资源基于标签选择器将一组pod定义成一个逻辑组合，并通过自己的IP地址和端口调度代理请求至组内的对象上。并对客户端隐藏了真实的处理用户请求的pod资源。Service资源会通过API Service持续监视着标签选择器匹配到的后端pod对象，并实时跟踪个对象的变动；但是service并不直接连接至pod对象，它们直接还有一个中间层（Endpoint）资源对象，创建Service资源对象时，其关联的Endpoint对象会自动创建。

（2）service实现方式

       一个service对象就是工作节点上的一些iptables或ipvs规则，用于将到达service对象的IP地址的流量调度转发至相应的Endpoint对象制定的IP地址和端口上。Service ip就是用于生成iptables或ipvs规则时使用的IP地址。Kube-proxy将请求代理至相应端点的方式主要有三种：

       1）userspace代理模型

       这种代理模型为kubernetes1.1版本之前默认的代理模型；这种代理模型中，请求在内核空间和用户空间来回转发导致效率不高。

       2）iptables代理模型

       这种调度模型为kubernetes1.2版本至1.11版本之间的默认调度模型；默认算法是随机调度；iptables代理模型不会再背挑选中的后端pod资源无响应时自动进行重定向。

       3）ipvs调度模型

       Ipvs调度模型从1.11版本开始成为默认的调度模型；这种模型中，ipvs构建于netfilter的钩子函数之上，使用hash表作为底层数据结构并工作与内核空间，具有流量转发速度快，规则同步性能好等特性。IPvs支持支持众多调度算法，如rr、lc、dh、sh、sed、nq等。

2、service资源的使用
       Service资源的定义与其他资源的定义方式基本相同，spec字段经常嵌套的的字段有selector（定义标签选择器）ports（定义要暴露的端口）。

# 为前面创建的deployment控制器创建定义service资源
[root@master01 test03]# cat test01.yaml 
apiVersion: v1
kind: Service
metadata:
  name: test-svc
spec:
  selector:
    app: test-deploy
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
# 创建service资源
[root@master01 test03]# kubectl apply -f test01.yaml 
service/test-svc created
# 查看创建的service
[root@master01 test03]# kubectl get svc test-svc
NAME       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
test-svc   ClusterIP   10.100.249.83   <none>        80/TCP    3m6s
# 查看endpoint资源端点列表
[root@master01 test03]# kubectl get endpoints test-svc
NAME       ENDPOINTS                                                  AGE
test-svc   10.244.1.38:80,10.244.1.39:80,10.244.2.37:80 + 2 more...   4m24s
3、服务发现
       在kubernetes集群中，service为pod中的服务类应用提供了一个稳定的访问入口，pod中的应用是通过服务发现机制来得知某个特定service资源的IP和端口的。

（1）服务发现

       根据服务发现的实现方式，可将服务发现分为客户端发现（由客户端到服务注册中心发现其依赖到的服务的相关信息）和服务端发现（通过中央路由器或者服务均衡器组件实现）两类。

       在kubernetes中，1.3版本开始服务发现是由kubeDNS实现的，从1.11版本后，服务发现功能是由CoreDNS组件实现的。

（2）服务发现的方式

       1）环境变量方式

       在创建pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的方式注入其中，支持使用kubernetes service环境变量及与docker的links兼容的变量。

       2）DNS的方式

       ClusterDNS是kubernetes系统中用于服务解析和名称发现的服务，集群中创建的每个service对象，都会由其生成相关的资源记录；默认集群内的各pod资源会自动配置其作为名称解析服务器，并在其DNS搜索列表中包含它所属名称空间的域名后缀。

       在创建service资源对象时，ClusterDNS会为它自动创建资源记录用于名称解析和服务注册，pod资源可直接使用标准的DNS名称来访问这些“servie”资源。

4、服务暴露
       Service的IP地址仅在集群内可达，而有些服务需要暴露到外部网络中接受各类客户端的访问，此时就需要在集群的边缘为其添加一层转发机制，以实现将外部请求接入到集群service资源之上，及将集群中的服务发布到外部网络中。Kubernetes中service共有四种类型，分别如下：

（1）ClusterIP

通过集群内部的IP地址暴露服务，地址仅在集群内部可达，无法被集群外部的客户端访问。

（2）NodePort

       NodePort类型建立在ClusterIP类型之上，用其所在的每个节点的IP地址的某静态端口暴露服务，它依然会为service分配集群IP地址，并将此作为NodePort的路由目标。

       Kubernetes集群在安装部署时通常会预留一个端口范围用于NodePort，默认为30000-322767之间的端口。而在使用NodePort类型的service时，需要通过” service.spec.type”字段指定service类型名称。

# 定义NodePort类型的service资源，并指定端口（一般不建议指定端口）
[root@master01 test03]# cat test02.yaml 
apiVersion: v1
kind: Service
metadata:
  name: test-nodportsvc
spec:
  type: NodePort
  selector:
    app: test-deploy
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30033
# 查看创建好的service，依然创建了clusterIP，并将容器内的端口映射到了node节点上
[root@master01 test03]# kubectl get svc test-nodportsvc
NAME              TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
test-nodportsvc   NodePort   10.111.192.243   <none>        80:30033/TCP   6m20s
（3）LoadBalancer

LoadBalancer类型建立在NodePort类型之上，其通过cloud provider提供的负载均衡器将服务暴露到集群外部。LoadBalancer类型的service与NodePort类型的service的使用方法基本类似。

（4）ExternalName

       ExternalNamel类型的service资源用于将集群外部的服务发布到集群中以供Pod中的应用程序访问，因此，不需要使用标签选择器关联任何的pod对象，但必须要用spec.externalName属性定义一个CNAME记录用于返回外部真正提供服务的主机的别名，而后通过CHAME记录获取到相关主机的IP地址。

5、Headless类型的Service资源
如果客户端需要直接访问Service资源后端的所有Pod资源，这时就应该向客户端暴露每个Pod资源的IP地址，而不再是中间层Service对象的ClusterIP，这种类型的service称为headless service。这种类型的service也没有使用负载均衡代理它的需要。

如何为Headless类型的Service资源配置IP地址取决于它的标签选择器的定义；如果具有标签选择器，则端点控制器会在API中为其创建Endpoint记录，并将ClusterDns服务中的A记录直接解析到此Service后端的各Pod对象的IP地址对象上；如果没有标签选择器，则端点控制器不会在API中为其创建Endpoints记录。

（1）创建Headless Service资源

       定义Headless Service类型的资源时，只需将ClusterIP字段的至设置为“None”即可。

# 定义Headless Service类型
[root@master01 test03]# cat test03.yaml 
kind: Service
apiVersion: v1
metadata: 
  name: test-headless-svc
spec:
  clusterIP: None
  selector:
    app: test-deploy
  ports:
  - name: httpport
    port: 80
    targetPort: 80
# 查看创建的headless service资源
[root@master01 test03]# kubectl get svc test-headless-svc
NAME                TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
test-headless-svc   ClusterIP   None         <none>        80/TCP    62s
[root@master01 test03]# kubectl describe test-headless-svc
error: the server doesn't have a resource type "test-headless-svc"
[root@master01 test03]# kubectl describe svc test-headless-svc
. . . . . . 
Endpoints:         10.244.1.38:80,10.244.1.39:80,10.244.2.37:80 + 2 more...
（2）pod资源发现

       Headless Service通过标签选择器关联到所有pod资源的IP地址之上，客户端向此service对象发起的请求将通过dns查询时返回的IP地址（多个IP地址则以轮询方式返回）直接接入到Pod资源中的应用之上，而不再由Service资源进行代理转发。

# 查看dns对此service名称的解析
[root@master01 test03]# kubectl exec -it cirros-58cc4cdc59-2f4s6 -- sh
/ # nslookup test-headless-svc
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local
 
Name:      test-headless-svc
Address 1: 10.244.2.39 10-244-2-39.test-headless-svc.default.svc.cluster.local
Address 2: 10.244.2.38 10-244-2-38.test-headless-svc.default.svc.cluster.local
Address 3: 10.244.2.37 10-244-2-37.test-headless-svc.default.svc.cluster.local
Address 4: 10.244.1.38 10-244-1-38.test-headless-svc.default.svc.cluster.local
Address 5: 10.244.1.39 10-244-1-39.test-headless-svc.default.svc.cluster.local
6、Ingress资源
       Kubernetes提供了两种负载均衡机制，一种是工作与传输层的Service资源，实现“TCP负载均衡器”，另一种是Ingress资源，实现“HTTP(S)负载均衡器”。

（1）Ingress和Ingress控制器

       Ingress是kubernetes API的标准类型资源之一，它其实是一组基于DNS名称或URL路劲把请求转发至指定的Service资源的规则，用于将集群外部的请求转发至集群内部完成服务的发布。

       Ingre资源并不能进行“能量穿透”，它仅是一组路由规则的集合，而能够为Ingress资源监听套接字并转发流量的组件称为Ingress控制器（Ingress Controller）。

       Ingress控制器可以由任何具有反向代理功能的服务程序实现，Ingress也是运行于集群中的Pod资源，应与被代理的pod运行与同一网络中。

（2）Ingress资源的定义

       Ingress资源是基于HTTP虚拟主机或URL的转发规则，Ingress.spec资源是定义Ingress资源的核心组成部分，它主要由以下嵌套字段组成：

           rules：用于定义当前Ingress资源的转发规则列表，未定义的rule规则或者匹配不到  任何规则时，所有的流量都会转发到backend定义的默认后端。

           backend：默认的后端用于服务那些没有匹配到任何规则的请求。Backend对象的   定义由两个必选的内嵌字段serviceName和servicePort组成。

           tls：目前仅支持通过默认端口443提供服务，tls也有以下两个内嵌字段组成，仅     在定义TLS的转发规则时才需要定义此类对象：

              hosts：包含于使用的TLS证书之内的主机名称字符串列表

             secretName：用于引用SSL回话的secret对象名称，在基于SNI实现多主机路由的场景中，此字段为可选。

# 定义ingress资源
[root@master01 test03]# cat test04.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata: 
  name: test-ingress
  annotations:
    kubernetes.io/ingress: “nginx test”
spec:
  rules:
  - host: www.dayi123.com
    http:
      paths:
      - backend:
          serviceName: test-headless-svc
          servicePort: 80
（3）Ingress资源的类型

       1）单service资源型ingress

        暴露单个服务的方法可以使用service的NodePort、LoadBalancer类型；也可使用Ingress,使用Ingress时，只需指定ingress的default backend即可。

# 定义一个单Service资源型的ingress
[root@master01 test03]# cat test05.yaml 
apiVersion: extensions/v1beat1
kind: Ingress
metadata:
  name: test-ingress02
spec:
  backend:
    serviceName: nginx-svc
    serviceName: 80
       2）基于URL路劲进行流量分发的ingress

        基于URL路径转发是根据客户端请求不同的URL路径转发到不同的后端服务中。

# 定义一个多路径的ingress资源
[root@master01 test03]# cat test06.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata: 
  name: test-ingress05
  annotations:
    kubernetes.io/ingress: “nginx test path”
spec:
  rules:
  - host: www.dayi123.com
    http:
      paths: 
      - path: /nginx
        backend:
          serviceName: nginx-svc
          servicePort: 80
      - path: /tomcat
        backend:
          serviceName: test-svc
          servicePort: 80
       3）基于主机名称的ingress虚拟主机

         基于主机名的ingress虚拟主机是将每个应用分别以独立的FQDN主机名进行输出。

# 定义一个基于主机名的service
[root@master01 test03]# cat test07.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata: 
  name: test-ingress06
spec:
  rules:
  - host: blog.dayi123.com
    http:
      paths:
      - backend:
          serviceName: nginx-svc
          servicePort: 80
  - host: doc.dayi123.com
    http:
      paths:
      - backend:
          serviceName: test-svc
          servicePort: 80
       4）TLS类型的Ingress资源

         TLS类型的Ingress资源用于以HTTPS发布Service资源，基于一个含有私钥和证书的Secret对象即可配置TLS协议的Ingress资源。Ingress资源目前仅支持单TLS端口，并且还会卸载TLS回话。

（4）部署基于nginx的Ingress控制器

         Ingress是自身运行于Pod中的容器应用，一般是nginx或Envoy一类的具有代理及负载均衡功能的守护进程，它监视着来自于API Service的Ingress对象状态，并以其规则生

       相应的应用程序专有格式的配置文件并通过重载或重启守护进程而使新配置生效。运行Pod资源的Ingress控制器接入外部请求有以下两种方法：

       1）以Deployment控制器管理Ingress控制器的Pod资源，并通过NodePort或LoadBalancer类型的Service对象为其接入集群外部的请求流量

       2）借助于DaemonSet控制器，将Ingress控制器的Pod资源各自以单一实力的方式运行于集群的所有或部分工作节点，并配置该类pod以hostPort或hostNetwork的方式在当前节点接入外部流量。

# 在线创建nginx ingress控制器
[root@master01 ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml
# 查看创建的nginx ingress控制器是否运行正常
[root@master01 ~]# kubectl get pods -n ingress-nginx
NAME                                       READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-c595c6896-6xcd5   1/1     Running   0          92m
       在线的nginx ingress配置清单中采用了基于deployment控制器部署方式，因此接入外部流量之前需要通过NodePort或LoadBalancer类型的service资源对象。

# 为nginx ingress定义service资源配置清单
[root@master01 test03]# cat test-nginx-ingress.yaml 
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  type: NodePort
  ports: 
    - name: http
      port: 80
    - name: https
      port: 443
  selector:
     app.kubernetes.io/name: ingress-nginx
# 为nginx ingress控制器创建service
[root@master01 test03]# kubectl apply -f test-nginx-ingress.yaml 
service/nginx-ingress-controller created
# 查看创建的service
[root@master01 test03]# kubectl get svc -n ingress-nginx
NAME                       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
nginx-ingress-controller   NodePort   10.105.134.97   <none>        80:30409/TCP,443:31235/TCP   16s
 

7、使用ingress发布Tomcat
       实验的拓扑如下



（1）创建名称空间

# 定义名称空间的资源配置清单
[root@master01 ingress-tomcat]# cat ingress-tomcat.yaml 
apiVersion: v1
kind: Namespace
metadata:
  name: test-ingress
  labels:
    env: test-ingress
# 创建名称空间
[root@master01 ingress-tomcat]# kubectl apply -f ingress-tomcat.yaml 
namespace/test-ingress created
（2）部署tomcat实例

# 定义基于deployment的tomcat资源配置清单
[root@master01 ingress-tomcat]# cat tomcat-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tomcat-deployment
  namespace: test-ingress
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ingress-tomcat
  template:
    metadata:
      labels:
        app: ingress-tomcat
    spec:
      containers:
      - name: tomcat
        image: tomcat
        ports:
        - name: httport
          containerPort: 8080
        - name: ajpport
          containerPort: 8009
# 创建tomcat的pod控制器
[root@master01 ingress-tomcat]# kubectl apply -f tomcat-deployment.yaml 
deployment.apps/tomcat-deployment created
# 查看创建的tomcat pod
[root@master01 ingress-tomcat]# kubectl get pods -n test-ingress
NAME                                 READY   STATUS    RESTARTS   AGE
tomcat-deployment-67f5dcbdb4-7mmhz   1/1     Running   0          96s
tomcat-deployment-67f5dcbdb4-km5q8   1/1     Running   0          96s
（3）创建Service资源

        Ingress资源仅能通过Service资源识别相应的Pod资源，获取其IP地址和端口，然后Ingress控制器即可直接使用各Pod对象的IP地址与Pod内的服务直接进行通信，不经过Service的代理和调度，因此Service资源的ClusterIP对Ingress控制器来说存不存在无所谓。

# 定义service的资源配置清单文件
[root@master01 ingress-tomcat]# cat tomcat-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: tomcat-svc
  namespace: test-ingress
  labels:
    app: tomcat-svc
spec:
  selector:
    app: ingress-tomcat
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP
# 创建并查看service
[root@master01 ingress-tomcat]# kubectl apply -f tomcat-service.yaml 
service/tomcat-svc created
[root@master01 ingress-tomcat]# kubectl get svc -n test-ingress
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
tomcat-svc   ClusterIP   10.106.153.248   <none>        80/TCP    15s
（4）创建Ingress资源

# 定义ingress资源配置清单
[root@master01 ingress-tomcat]# cat tomcat-ingress.yaml 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: tomcat
  namespace: test-ingress
  annotations:
    kubernetes.io/ingress: "nginx"
spec:
  rules:
  - host: tomcat.dayi123.com
    http:
      paths:
      - path:
        backend:
          serviceName: tomcat-svc
          servicePort: 80
# 创建并查看Ingress
[root@master01 ingress-tomcat]# kubectl apply -f tomcat-ingress.yaml 
ingress.extensions/tomcat created
[root@master01 ingress-tomcat]# kubectl get ingress -n test-ingress
NAME     HOSTS                ADDRESS   PORTS   AGE
tomcat   tomcat.dayi123.com             80      14s
        上面的配置完成后，在本地主机的hosts中添加tomcat.dayi123.com对应node节点的解析后，就可在本地通过http://tomcat.dayi123.com:30409去访问tomcat服务。而各node节点的30409端口是在前面为nginx类型的ingress控制器创建的service中定义的。

（5）配置TLS Ingress资源

         互联网中的服务基本都是以https的方式提供服务的，如果希望ingress控制器接受客户端的请求时又希望它能够提供https服务，就应该配置tls类型的ingress资源。

# 生成用于测试的私钥和自签证书
[root@master01 ingress-tomcat]# openssl genrsa -out tls.key 2048
[root@master01 ingress-tomcat]# openssl req -new -x509 -key tomcat.key -out tomcat.crt -subj /C=CN/ST=Shanghai/L=Shanghai/O/dev/CN=tomcat.dayi123.com -days 736
        在ingress控制器上配置HTTPS主机时，是不能直接使用私钥和证书文件的，而是要使用Secret资源对象来传递相关的数据。

# 创建一个TLS类型名为tomcat-ingress-secret的secret资源
[root@master01 ingress-tomcat]# kubectl create secret tls tomcat-ingress-secret --cert=tomcat.crt --key=tomcat.key -n test-ingress
secret/tomcat-ingress-secret created
# 查看创建的secret资源
[root@master01 ingress-tomcat]# kubectl get secrets tomcat-ingress-secret -n test-ingress
NAME                    TYPE                DATA   AGE
tomcat-ingress-secret   kubernetes.io/tls   2      109s
        secret资源创建完成后，就可以将创建的secret应用到ingress资源的配置清单中。

# 定义TLS类型的Ingress资源的配置清单
[root@master01 ingress-tomcat]# cat tomcat-ingress-tls.yaml 
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: tomcat-ingress-tls
  namespace: test-ingress
  annotations:
    kubernetes.io/ingress: "nginx"
spec:
  tls:
  - hosts:
    - tomcat.dayi123.com
    secretName: tomcat-ingress-secret
  rules:
  - host: tomcat.dayi123.com
    http:
      paths:
      - path:
        backend:
          serviceName: tomcat-svc
          servicePort: 80
# 创建并查看tls类型的ingress资源
[root@master01 ingress-tomcat]# kubectl apply -f tomcat-ingress-tls.yaml 
ingress.extensions/tomcat-ingress-tls created
[root@master01 ingress-tomcat]# kubectl get ingress tomcat-ingress-tls -n test-ingress
NAME                 HOSTS                ADDRESS   PORTS     AGE
tomcat-ingress-tls   tomcat.dayi123.com             80, 443   3m35s
（6）测试

        在前面为基于nginx的ingress控制器创建的service中已将443端口映射到了node节点的31235端口，在本地对域名tomcat.dayi123.com做了解析就可以通过https://tomcat.dayi123.com: 31235进行测试。

# 查看ingress控制器中映射到本地的端口
[root@master01 ingress-tomcat]# kubectl get svc -n ingress-nginx
NAME                       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
nginx-ingress-controller   NodePort   10.105.134.97   <none>        80:30409/TCP,443:31235/TCP   167m
# 在linux客户端进行测试（测试前要做hosts解析）
[root@master01 ingress-tomcat]# curl -k -v https://tomcat.dayi123.com:31235
————————————————
版权声明：本文为CSDN博主「dayi_123」的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/dayi_123/article/details/89703961

想要访问到pod中的服务, 最简单的方式就是通过端口转发, 执行如下命令, 将宿主机的9999端口与nginx-pod的80端口绑定:
[root@nas-centos1 ~]$ kubectl port-forward --address 0.0.0.0 nginx-pod 9999:80
Forwarding from 0.0.0.0:9999 -> 80
Handling connection for 9999 


# 定义一个多路径的ingress资源
[root@master01 test03]# cat test06.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata: 
  name: test-ingress05
  annotations:
    kubernetes.io/ingress: “nginx test path”
spec:
  rules:
  - host: www.dayi123.com
    http:
      paths: 
      - path: /nginx
        backend:
          serviceName: nginx-svc
          servicePort: 80
      - path: /tomcat
        backend:
          serviceName: test-svc 
