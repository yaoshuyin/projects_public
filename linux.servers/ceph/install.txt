.架构
...................................
1) 192.168.0.51 ceph-node01 
   ceph-deploy mon osd

2) 192.168.0.52 ceph-node02 
   mon osd

3) 192.168.0.53 ceph-node03 
   mon osd

.主机名
...................................
 ceph-node01 ceph-node02 ceph-node03
 
./etc/hosts (三台都加)
...................................
192.168.0.51 ceph-node01
192.168.0.52 ceph-node02
192.168.0.53 ceph-node03

.准备(三台都执行)
...................................
1)关闭防火墙 或 开放端口:  Monitors之间默认使用 6789 端口通信， OSD之间默认用 6800:7300 内的端口通信
systemctl stop firewalld
systemctl disable firewalld

或
firewall-cmd --zone=public --add-port=6789/tcp --permanent
firewall-cmd --zone=public --add-port=6800-7100/tcp --permanent
firewall-cmd --reload
firewall-cmd --zone=public --list-all

2)安装ntp服务
yum -y install ntp
systemctl enable ntpd
systemctl start ntpd 

3)禁用SELINUX
vim /etc/selinux/config
SELINUX=disabled

reboot


4) 安装软件 (三台)
4.1) vim /etc/yum.conf
proxy=http://192.168.0.9:8889

4.ceph源
vim /etc/yum.repos.d/ceph.repo
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-mimic/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-mimic/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-mimic/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

5)更新安装
yum update
yum upgrade
yum -y install yum-plugin-priorities  --enablerepo=rhel-7-server-optional-rpms
yum -y install gcc python-setuptools python-devel
yum -y install ceph-deploy centos-release-ceph-luminous ceph-common


.免密登录(ceph-node01上执行)
...................................
ssh-keygen -t rsa -b 2048 -C ceph-node01
ssh-copy-id -i ~/.ssh/id_rsa.pub ceph-node02
ssh-copy-id -i ~/.ssh/id_rsa.pub ceph-node03

.部署集群(ceph-node01)
...................................
1)创建目录
mkdir -p /opt/ceph-cluster
cd /opt/ceph-cluster

2)新建立集群 (node名字 要和主机名一样)
ceph-deploy --overwrite-conf new ceph-node01 ceph-node02 ceph-node03

3)安装
ceph-deploy --overwrite-conf install --release mimic ceph-node01 ceph-node02 ceph-node03

4)创建monitor
ceph-deploy  --overwrite-conf  mon create-initial

5)配置admin key
ceph-deploy  --overwrite-conf admin ceph-node01 ceph-node02 ceph-node03

6)添加osd
查看可用磁盘
ceph-deploy disk list ceph-node01
ceph-deploy disk list ceph-node02
ceph-deploy disk list ceph-node03

删除磁盘分区表和内容
ceph-deploy disk zap ceph-node01:sdc ceph-node02:sdc ceph-node03:sdc

使用create命令格式化磁盘为数据和日志两个分区
ceph-deploy osd create ceph-node01 --data /dev/sdc
ceph-deploy osd create ceph-node02 --data /dev/sdc
ceph-deploy osd create ceph-node03 --data /dev/sdc

7)其他节点添加monitor
ceph-deploy --overwrite-conf mon add ceph-node02
ceph-deploy --overwrite-conf mon add ceph-node03
  
8)添加管理进程
ceph-deploy --overwrite-conf mgr create ceph-node01

9)添加元数据服务
 ceph-deploy mds create ceph-node01
  
10)查看状态
$ ceph health
HEALTH_OK

$ ceph status
  cluster:
    id:     06dabeca-804a-4eed-86e9-f56792a5e3f9
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph-node01,ceph-node02,ceph-node03
    mgr: ceph-node01(active)
    osd: 3 osds: 3 up, 3 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   3.0 GiB used, 177 GiB / 180 GiB avail
    pgs:     

11)查看仲裁状态
 $ ceph quorum_status --format json-pretty
 
12)查看集群使用状态
 $ ceph df
 
13)查看mon osd pg状态
 $ ceph mon stat
 $ ceph osd stat
 $ ceph pg stat

..................................................

1)创建pool,名称为rbd
  ceph osd pool create rbd 128
  ceph osd lspools
  rados df
 
2)在pool rbd上创建块设备rbd1
  rbd create rbd1 --size 1024 -p rbd
  ceph osd pool get rbd size

3)映射rbd1到/dev/rbdx,以生成可用磁盘
 $ rbd map --image rbd1 
 /dev/rbd0 created
 
 创建文件系统
 $ mkfs.xfs /dev/rbd0
 $ mkdir /mnt/rbd0test
 $ mount /dev/rbd0 /mnt/rbd0test
 $ df -h
 $ cd /mnt/rbd0test/
 $ touch 1.txt
 $ ls

4)rbd创建快照
$ rbd snap create rbd/rbd1@snap.001.202102012255
$ rbd snap ls rbd/rbd1
SNAPID NAME                   SIZE TIMESTAMP                
     4 snap.001.202102012255 1 GiB Mon Feb  1 22:55:30 2021 
